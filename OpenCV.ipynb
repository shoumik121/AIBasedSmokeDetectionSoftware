{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OpenCV.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNZPOGVZlDIF9XLVrEgkHV3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fLBuOs7UF_SY","executionInfo":{"status":"error","timestamp":1622903696716,"user_tz":-360,"elapsed":2476,"user":{"displayName":"Shoumik Rouf","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfFU4KcAnO4XnECGgWdNIIejh8OVdZsRDgwuL0rg=s64","userId":"02798590850560546307"}},"outputId":"efe06371-7648-45cf-b843-f60899ba20fb","colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["import numpy as np\n","import cv2\n"," \n","# TODO: This code uses the Gaussian mixture model in the OpenCV interface to detect moving targets.\n"," \n","cap = cv2.VideoCapture(0) # Turn on the camera\n","                          # cap = cv2.VideoCapture(\"video.mp4\")\n"," \n","size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n"," \n","# cap = cv2.VideoCapture(\"surveillance_demo\\\\demo.mp4\")\n","Mog = cv2.createBackgroundSubtractorMOG2() # Define Gaussian mixture model object mog\n","fourcc1 = cv2.VideoWriter_fourcc(*'XVID')\n","fourcc2 = cv2.VideoWriter_fourcc(*'XVID')\n","out_detect = cv2.VideoWriter('output_detect.avi', fourcc1, 20.0, size)\n","out_bg = cv2.VideoWriter('output_bg.avi', fourcc1, 20.0, size)\n","i = 0\n","while(1):\n","  Ret, frame = cap.read() # Read the image of each frame of the camera, frame is the image of this frame\n","print(frame.shape)\n","Fgmask = mog.apply(frame) # Use the Gaussian mixture model object defined above to mog the moving target detection of the current frame, returning the binary image\n","gray_frame = fgmask.copy()\n","         # Use findContours to detect image outline boxes. The specific principles are papers, but it is not recommended. Will use it.\n","         # Return value: image, outline image, not commonly used. Cnts, the coordinates of the outline. Hierarchy, parent-child relationship between boxes, not commonly used.\n","image, cnts, hierarchy = cv2.findContours(gray_frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","         # Draw each cnts box into the original image frame\n","for c in cnts:\n","           if cv2.contourArea(c) < 900: # Calculate the area of ​​the candidate box. If it is less than 1500, skip the current candidate box.\n","              continue \n","           (x, y, w, h) = cv2.boundingRect(c), # According to the outline c, get the current best rectangular frame\n","           Cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 255, 0), 2) # draw the rectangle on the current frame frame\n","           Cv2.rectangle(image, (x, y), (x + w, y + h), (255, 255, 255), 2) # draw the rectangle on the current frame frame\n"," \n","Cv2.imshow(\"contours\", frame) # Display the current frame\n","Cv2.imshow(\"fgmask\", image) # Display the foreground image of the motion\n","image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n","print(image.shape)\n","print(frame.shape)\n"," \n","out_detect.write(frame)\n","out_bg.write(image)\n","cv2.waitKey(20)\n","i = i+1\n","          # if cv2.waitKey(int(1000 / 12)) & 0xff == ord(\"q\"): # You don't have to look at it, this is the interrupt mechanism provided by cv\n","    #     break\n"," \n","Cap.release() # release candidate box\n","out_detect.release()\n","out_bg.release()\n","cv2.destroyAllWindows()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-399ee56ca74a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mRet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Read the image of each frame of the camera, frame is the image of this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mFgmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use the Gaussian mixture model object defined above to mog the moving target detection of the current frame, returning the binary image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}